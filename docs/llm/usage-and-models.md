# Usos de LLM, Jobs y Modelos

## Descripci√≥n General

El sistema utiliza modelos de lenguaje grande (LLM) de Google Gemini para diversas tareas relacionadas con el an√°lisis de algoritmos. La configuraci√≥n est√° centralizada y cada "job" (tarea) tiene asignado un modelo espec√≠fico optimizado para ese prop√≥sito.

## Configuraci√≥n Centralizada

**Ubicaci√≥n**: `apps/web/src/app/api/llm/llm-config.ts`

### Estructura de Configuraci√≥n

```typescript
export type LLMJob =
  | "classify"        // ‚ö†Ô∏è LEGACY - No se usa, clasificaci√≥n es por heur√≠stica
  | "parser_assist"   // Asistencia para generar/corregir c√≥digo
  | "general"         // Chatbot general
  | "simplifier"      // Simplificaci√≥n matem√°tica
  | "repair"          // Reparaci√≥n de c√≥digo con errores
  | "compare";        // Comparaci√≥n de an√°lisis

export const GEMINI_MODELS = {
  classify: "gemini-2.0-flash-lite",      // ‚ö†Ô∏è LEGACY - No se usa
  parser_assist: "gemini-2.5-flash",
  general: "gemini-2.5-flash",
  simplifier: "gemini-2.5-flash",
  repair: "gemini-2.5-flash",
  compare: "gemini-2.5-pro",
};
```

## Jobs Disponibles

### ‚ö†Ô∏è LEGACY: Classify (NO SE USA)

**Estado**: **DEPRECADO** - No se utiliza en producci√≥n

**Raz√≥n**: La clasificaci√≥n de algoritmos se realiza completamente por **heur√≠stica** en el backend Python mediante el endpoint `/classify`. No se usa LLM para esta tarea.

**Modelo anterior**: `gemini-2.0-flash-lite`

**Nota**: Aunque el job existe en la configuraci√≥n, **no hay ning√∫n endpoint activo** que lo use. La clasificaci√≥n es 100% determinista y basada en an√°lisis del AST.

---

### 1. Parser Assist

**Prop√≥sito**: Generar c√≥digo de algoritmos y asistir en correcci√≥n de sintaxis

**Modelo**: `gemini-2.5-flash`

**Configuraci√≥n**:
- `maxTokens`: 16000
- `temperature`: 0.7 (creativo pero coherente)

**Uso**:
```typescript
// apps/web/src/components/ChatBot.tsx

// Cuando el usuario pide c√≥digo o implementaci√≥n
const response = await fetch("/api/llm", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "X-API-Key": apiKey
  },
  body: JSON.stringify({
    job: "parser_assist",
    message: "Dame el c√≥digo de merge sort"
  })
});
```

**Caracter√≠sticas**:
- Genera c√≥digo en la gram√°tica del proyecto (Language.g4)
- Corrige errores de sintaxis
- Convierte descripciones a pseudoc√≥digo v√°lido
- Proporciona ejemplos de algoritmos

**Endpoint**: `POST /api/llm` (con `job: "parser_assist"`)

### 2. General

**Prop√≥sito**: Chatbot general para explicaciones y consultas te√≥ricas

**Modelo**: `gemini-2.5-flash`

**Configuraci√≥n**:
- `maxTokens`: 16000
- `temperature`: 0.7 (conversacional)

**Uso**:
```typescript
// apps/web/src/components/ChatBot.tsx

const response = await fetch("/api/llm", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "X-API-Key": apiKey
  },
  body: JSON.stringify({
    job: "general",
    message: "¬øQu√© es el teorema maestro?",
    history: previousMessages
  })
});
```

**Caracter√≠sticas**:
- Explica conceptos de algoritmos
- Analiza complejidad temporal y espacial
- Responde preguntas sobre programaci√≥n
- Mantiene contexto de conversaci√≥n

**Endpoint**: `POST /api/llm` (con `job: "general"`)

### 3. Simplifier

**Prop√≥sito**: Simplificar expresiones matem√°ticas de an√°lisis

**Modelo**: `gemini-2.5-flash`

**Configuraci√≥n**:
- `maxTokens`: 8000
- `temperature`: 0 (completamente determinista)

**Uso**:
```typescript
// Usado internamente por el backend durante an√°lisis

// El backend llama a este job para simplificar sumatorias
// y expresiones algebraicas complejas
```

**Caracter√≠sticas**:
- Simplifica sumatorias a formas cerradas
- Elimina par√©ntesis innecesarios
- Agrupa t√©rminos similares
- Genera forma polin√≥mica T(n) = an¬≤ + bn + c
- Respeta notaci√≥n original (n vs N)

**Uso**: Interno del backend, no expuesto directamente al frontend

### 4. Repair

**Prop√≥sito**: Reparar c√≥digo con errores de sintaxis

**Modelo**: `gemini-2.5-flash`

**Configuraci√≥n**:
- `maxTokens`: 16000
- `temperature`: 0.5 (balance entre determinismo y creatividad)

**Uso**:
```typescript
// apps/web/src/components/RepairModal.tsx

const response = await fetch("/api/llm", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "X-API-Key": apiKey
  },
  body: JSON.stringify({
    job: "repair",
    code: codeWithErrors,
    errors: parseErrors
  })
});
```

**Caracter√≠sticas**:
- Corrige errores de sintaxis
- Mantiene la l√≥gica original
- Reporta l√≠neas modificadas/eliminadas
- Respeta reglas de la gram√°tica

**Endpoint**: `POST /api/llm` (con `job: "repair"`)

**Response**:
```typescript
{
  "code": "c√≥digo corregido",
  "removedLines": [3, 5],
  "addedLines": [4, 6, 7]
}
```

### 5. Compare

**Prop√≥sito**: Comparar an√°lisis del sistema con an√°lisis del LLM

**Modelo**: `gemini-2.5-pro` (modelo m√°s potente para an√°lisis preciso)

**Configuraci√≥n**:
- `maxTokens`: 8000
- `temperature`: 0.1 (muy determinista)

**Uso**:
```typescript
// apps/web/src/app/analyzer/page.tsx

const response = await fetch('/api/llm', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'X-API-Key': apiKey
  },
  body: JSON.stringify({
    job: 'compare',
    systemAnalysis: analysisResult,
    source: code
  })
});
```

**Caracter√≠sticas**:
- Valida correcci√≥n matem√°tica del an√°lisis
- Proporciona an√°lisis independiente
- Detecta discrepancias
- Genera nota de validaci√≥n (‚â§100 caracteres)

**Endpoint**: `POST /api/llm` (con `job: "compare"`)

**Response**:
```typescript
{
  "analysis": {
    "worst": {
      "T_open": "...",
      "T_polynomial": "...",
      "big_o": "O(n¬≤)",
      "big_omega": "Œ©(n¬≤)",
      "big_theta": "Œò(n¬≤)"
    }
  },
  "note": "üòä Excelente, T_open y cotas correctas"
}
```

**Documentaci√≥n**: [LLM Comparison](../app/llm-comparison.md)

### 6. Recursion Diagram (Adicional)

**Prop√≥sito**: Generar diagramas de √°rbol de recursi√≥n

**Modelo**: `gemini-2.0-flash` (usado en endpoint espec√≠fico)

**Uso**:
```typescript
// apps/web/src/components/RecursionTreeView.tsx

const response = await fetch("/api/llm/recursion-diagram", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "X-API-Key": apiKey
  },
  body: JSON.stringify({
    source: code,
    inputs: { n: 5 }
  })
});
```

**Caracter√≠sticas**:
- Genera diagramas Mermaid de √°rboles de recursi√≥n
- Incluye explicaci√≥n del proceso
- Proporciona valores de entrada/salida de cada llamada

**Endpoint**: `POST /api/llm/recursion-diagram`

**Documentaci√≥n**: [Pseudocode Tracking](../app/pseudocode-tracking.md)

## Modelos de Gemini Usados

### Gemini 2.5 Flash

**Usado en**: `parser_assist`, `general`, `simplifier`, `repair`

**Caracter√≠sticas**:
- R√°pido y eficiente
- Bueno para tareas de generaci√≥n y an√°lisis
- Costo moderado
- L√≠mite de contexto: 1M tokens

**Precios** (a diciembre 2024):
- Input: $0.075 por mill√≥n de tokens
- Output: $0.30 por mill√≥n de tokens

### Gemini 2.5 Pro

**Usado en**: `compare`

**Caracter√≠sticas**:
- M√°s potente y preciso
- Mejor para an√°lisis matem√°tico complejo
- Costo m√°s alto
- L√≠mite de contexto: 2M tokens

**Precios**:
- Input: $1.25 por mill√≥n de tokens
- Output: $5.00 por mill√≥n de tokens

### Gemini 2.0 Flash (Lite)

**Usado en**: `recursion-diagram` (endpoint espec√≠fico)

**Caracter√≠sticas**:
- Versi√≥n ligera y r√°pida
- Bueno para generaci√≥n de diagramas
- Costo bajo

**Nota**: El job `classify` est√° configurado con `gemini-2.0-flash-lite` pero **NO SE USA** en producci√≥n.

## Endpoints de LLM

### Estructura de Endpoints

```
/api/llm/
‚îú‚îÄ‚îÄ route.ts              # Endpoint principal (POST /api/llm)
‚îú‚îÄ‚îÄ recursion-diagram/    # Generaci√≥n de diagramas recursivos
‚îî‚îÄ‚îÄ status/               # Validaci√≥n de API key
```

### Endpoint Principal: POST /api/llm

**Request**:
```typescript
{
  "job": "parser_assist" | "general" | "simplifier" | "repair" | "compare",
  "message"?: string,      // Para parser_assist y general
  "history"?: Message[],   // Para general (contexto del chat)
  "code"?: string,         // Para repair
  "errors"?: Error[],      // Para repair
  "systemAnalysis"?: any,  // Para compare
  "source"?: string        // Para compare
}
```

**Response**:
```typescript
{
  "text": string,          // Respuesta del LLM
  "tokensUsed": number,
  "cost": number
}
```

### Middleware Com√∫n

```typescript
// apps/web/src/app/api/llm/route.ts

export async function POST(request: Request) {
  // Obtener API key (prioridad: header > env)
  const apiKey = request.headers.get('X-API-Key') || 
                 process.env.GEMINI_API_KEY;

  if (!apiKey) {
    return Response.json(
      { error: 'API key required' },
      { status: 401 }
    );
  }

  // Obtener configuraci√≥n del job
  const { job } = await request.json();
  const config = getJobConfig(job);

  // Llamar a Gemini
  const result = await callGeminiLLM(config, request, apiKey);

  return Response.json(result);
}
```

## Flujo de Uso en Diferentes Funcionalidades

### 1. Chatbot Interactivo

```
Usuario env√≠a mensaje
    ‚Üì
Clasificaci√≥n de intenci√≥n (local, sin LLM)
    ‚Üì
Si es "parser_assist" ‚Üí POST /api/llm (job: parser_assist)
Si es "general" ‚Üí POST /api/llm (job: general)
    ‚Üì
LLM procesa con contexto
    ‚Üì
Retorna respuesta conversacional
```

### 2. Reparaci√≥n de C√≥digo

```
Usuario tiene c√≥digo con errores
    ‚Üì
Abre RepairModal
    ‚Üì
POST /api/llm (job: repair)
    ‚Üì
LLM corrige sintaxis
    ‚Üì
Retorna c√≥digo corregido + l√≠neas modificadas
```

### 3. Comparaci√≥n de An√°lisis

```
Usuario solicita comparaci√≥n
    ‚Üì
POST /api/llm (job: compare)
    ‚Üì
LLM analiza c√≥digo independientemente
    ‚Üì
Compara con an√°lisis del sistema
    ‚Üì
Retorna an√°lisis + nota de validaci√≥n
```

### 4. Generaci√≥n de Diagramas Recursivos

```
Usuario solicita trace de algoritmo recursivo
    ‚Üì
POST /api/llm/recursion-diagram
    ‚Üì
LLM genera diagrama Mermaid
    ‚Üì
Construye √°rbol de recursi√≥n
    ‚Üì
Retorna visualizaci√≥n
```

## Variables de Entorno Relacionadas

### Frontend (Next.js)

```env
# .env.local

# API key del servidor (fallback si usuario no proporciona la suya)
GEMINI_API_KEY=AIza...

# Configuraci√≥n de LLM
NEXT_PUBLIC_DEFAULT_MODEL=gemini-2.5-flash
NEXT_PUBLIC_MAX_TOKENS=16000
```

### Prioridad de API Keys

1. **API key del usuario** (header `X-API-Key`) - Prioridad m√°s alta
2. **API key del servidor** (`GEMINI_API_KEY` en env) - Fallback

## Monitoreo y Logging

### Logging de Uso

```typescript
// Cada llamada a LLM registra:
console.log('[LLM Usage]', {
  job: 'parser_assist',
  model: 'gemini-2.5-flash',
  tokens: 1250,
  cost: '$0.00125',
  success: true
});
```

### M√©tricas T√≠picas

```typescript
const metrics = {
  parser_assist: {
    avgTokens: 2500,
    avgCost: 0.0025,
    avgLatency: 1800  // ms
  },
  general: {
    avgTokens: 1200,
    avgCost: 0.0012,
    avgLatency: 1200
  },
  compare: {
    avgTokens: 3500,
    avgCost: 0.0175,  // Pro es m√°s caro
    avgLatency: 2500
  }
};
```

## Optimizaciones

### Cache de Resultados

```typescript
// Cache en memoria para resultados de simplifier
const cache = new Map<string, CachedResult>();

export async function callGeminiWithCache(
  config: JobConfig,
  prompt: string,
  apiKey: string
): Promise<GeminiResult> {
  const cacheKey = `${config.job}_${hashPrompt(prompt)}`;

  const cached = cache.get(cacheKey);
  if (cached && !isExpired(cached)) {
    return cached.result;
  }

  const result = await callGeminiLLM(config, prompt, apiKey);
  
  cache.set(cacheKey, {
    result,
    timestamp: Date.now(),
    ttl: 3600000  // 1 hora
  });

  return result;
}
```

### Rate Limiting

```typescript
// L√≠mite de 60 requests por minuto por API key
const rateLimiter = new Map<string, number[]>();

export function checkRateLimit(apiKey: string): boolean {
  const now = Date.now();
  const requests = rateLimiter.get(apiKey) || [];

  const recentRequests = requests.filter(
    time => now - time < 60000
  );

  if (recentRequests.length >= 60) {
    return false;
  }

  recentRequests.push(now);
  rateLimiter.set(apiKey, recentRequests);

  return true;
}
```

## Resumen de Jobs Activos vs Legacy

### ‚úÖ Jobs Activos (En Uso)

| Job | Modelo | Uso | Endpoint |
|-----|--------|-----|----------|
| `parser_assist` | gemini-2.5-flash | Generaci√≥n de c√≥digo | `/api/llm` |
| `general` | gemini-2.5-flash | Chatbot general | `/api/llm` |
| `simplifier` | gemini-2.5-flash | Simplificaci√≥n matem√°tica | Interno backend |
| `repair` | gemini-2.5-flash | Reparaci√≥n de c√≥digo | `/api/llm` |
| `compare` | gemini-2.5-pro | Comparaci√≥n de an√°lisis | `/api/llm` |
| (recursion_diagram) | gemini-2.0-flash | Diagramas recursivos | `/api/llm/recursion-diagram` |

### ‚ùå Jobs Legacy (NO Se Usan)

| Job | Modelo | Estado | Raz√≥n |
|-----|--------|--------|-------|
| `classify` | gemini-2.0-flash-lite | **DEPRECADO** | Clasificaci√≥n es por heur√≠stica en `/classify` (backend Python) |

## Referencias

- [API Key Configuration](../app/api-key-configuration.md) - Configuraci√≥n de API key
- [LLM Comparison](../app/llm-comparison.md) - Comparaci√≥n con an√°lisis de LLM
- [Pseudocode Tracking](../app/pseudocode-tracking.md) - Seguimiento de ejecuci√≥n
- [Request Flow](../development/request-flow.md) - Flujo de peticiones
- [Google Gemini API Documentation](https://ai.google.dev/docs)
